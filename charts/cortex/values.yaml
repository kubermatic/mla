#  Container image settings.
#  Since the image is unique for all microservices, so are image settings.

image:
  repository: quay.io/cortexproject/cortex
  tag: v1.9.0
  pullPolicy: IfNotPresent

  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistrKeySecretName

# Kubernetes cluster DNS domain
clusterDomain: cluster.local

tags:
  blocks-storage-memcached: false

ingress:
  enabled: false
  annotations:
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - /
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

serviceAccount:
  create: true
  name:
  annotations: {}

useExternalConfig: false
externalConfigSecretName: 'secret-with-config.yaml'
externalConfigVersion: '0'

config:
  auth_enabled: false
  api:
    prometheus_http_prefix: '/prometheus'
    response_compression_enabled: true
  ingester:
    max_transfer_retries: 0
    lifecycler:
      join_after: 0s
      final_sleep: 0s
      num_tokens: 512
      ring:
        replication_factor: 3
        kvstore:
          store: consul
          prefix: 'collectors/'
          consul:
            host: 'consul:8500'
            http_client_timeout: '20s'
            consistent_reads: true
  limits:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h
    max_query_lookback: 0s
  schema:
    configs:
      - from: 2020-11-01
        store: cassandra
        object_store: cassandra
        schema: v10
        index:
          prefix: index_
          period: 168h
        chunks:
          prefix: chunks_
          period: 168h
  server:
    http_listen_port: 8080
    grpc_listen_port: 9095
    grpc_server_max_recv_msg_size: 104857600
    grpc_server_max_send_msg_size: 104857600
    grpc_server_max_concurrent_streams: 1000
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 104857600
      max_send_msg_size: 104857600
  # See https://github.com/cortexproject/cortex/blob/master/docs/configuration/config-file-reference.md#storage_config
  storage:
    engine: chunks
    cassandra:
      addresses:                  # configure cassandra addresses here.
      keyspace: cortex            # configure desired keyspace here.
      auth: true
      username:                   # configure cassandra user here.
      password:                   # configure cassandra password here.
    azure:
      container_name:             # configure azure blob container name here.
      account_name:               # configure azure storage account name here.
      account_key:                # configure azure storage account key here.
    # aws:
    #   dynamodb:
    #     dynamodb_url:
    #     api_limit:
    #     throttle_limit:
    #     metrics:
    #       url:
    #       target_queue_length:
    #       scale_up_factor:
    #       ignore_throttle_below:
    #       queue_length_query:
    #       write_throttle_query:
    #       write_usage_query:
    #       read_usage_query:
    #       read_error_query:
    #     chunk_gang_size:
    #     chunk_get_max_parallelism:
    #   s3:
    #   bucketnames:
    #   s3forcepathstyle:
    index_queries_cache_config:
      memcached:
        expiration: 1h
      memcached_client:
        timeout: 1s
  chunk_store:
    chunk_cache_config:
      memcached:
        expiration: 1h
      memcached_client:
        timeout: 1s
  table_manager:
    retention_deletes_enabled: false
    retention_period: 0s
  distributor:
    shard_by_all_labels: true
    pool:
      health_check_ingesters: true
  memberlist:
    bind_port: 7946
    join_members: []
    ## add here the service name of the memberlist
    ## if using memberlist discovery, eg:
    #  - cortex-memberlist
  querier:
    active_query_tracker_dir: /data/cortex/querier
    query_ingesters_within: 12h
  query_range:
    split_queries_by_interval: 24h
    align_queries_with_step: true
    cache_results: true
    results_cache:
      cache:
        memcached:
          expiration: 1h
        memcached_client:
          timeout: 1s
  ruler:
    enable_alertmanager_discovery: false
  alertmanager:
    external_url: '/api/prom/alertmanager'
  frontend:
    #  max_outstanding_per_tenant: 1000
    log_queries_longer_than: 10s

alertmanager:
  enabled: true
  replicas: 1

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This is useful for using a persistent volume for storing silences between restarts
    ##
    enabled: false

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 200m
  #    memory: 256Mi
  #  requests:
  #    cpu: 10m
  #    memory: 32Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity: {}
  annotations: {}

  ## DEPRECATED: use persistentVolume.subPath instead
  persistence:
    subPath:

  persistentVolume:
    ## If true and alertmanager.statefulSet.enabled is true,
    ## Alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## Alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Alertmanager data Persistent Volume size
    ##
    size: 2Gi

    ## Subdirectory of Alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ''

    ## Alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext:
    {}
    # fsGroup: 10001
    # runAsGroup: 10001
    # runAsNonRoot: true
    # runAsUser: 10001

  ## Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # If not set then a PodDisruptionBudget will not be created
  podDisruptionBudget:
    maxUnavailable: 1

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  statefulStrategy:
    type: RollingUpdate

  terminationGracePeriodSeconds: 60

  initContainers: []
  ## Init containers to be added to the cortex pod.
  # - name: my-init-container
  #   image: busybox:latest
  #   command: ['sh', '-c', 'echo hello']

  extraContainers: []
  ## Additional containers to be added to the cortex pod.
  # - name: reverse-proxy
  #   image: angelbarrera92/basic-auth-reverse-proxy:dev
  #   args:
  #     - "serve"
  #     - "--upstream=http://localhost:3100"
  #     - "--auth-config=/etc/reverse-proxy-conf/authn.yaml"
  #   ports:
  #     - name: http
  #       containerPort: 11811
  #       protocol: TCP
  #   volumeMounts:
  #     - name: reverse-proxy-auth-config
  #       mountPath: /etc/reverse-proxy-conf

  extraVolumes: []
  ## Additional volumes to the cortex pod.
  # - name: reverse-proxy-auth-config
  #   secret:
  #     secretName: reverse-proxy-auth-config

  ## Extra volume mounts that will be added to the cortex container
  extraVolumeMounts: []

  extraPorts: []
  ## Additional ports to the cortex services. Useful to expose extra container ports.
  # - port: 11811
  #   protocol: TCP
  #   name: http
  #   targetPort: http

  # Extra env variables to pass to the cortex container
  env: []

distributor:
  replicas: 2

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 100m
  #    memory: 512Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - distributor
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 60

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

ingester:
  replicas: 3

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This is useful when using WAL
    ##
    enabled: false

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 100m
  #    memory: 512Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    #  log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - ingester
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}

  ## DEPRECATED: use persistentVolume.subPath instead
  persistence:
    subPath:

  persistentVolume:
    ## If true and ingester.statefulSet.enabled is true,
    ## Ingester will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Ingester data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## Ingester data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Ingester data Persistent Volume size
    ##
    size: 2Gi

    ## Subdirectory of Ingester data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ''

    ## Ingester data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  startupProbe:
    # WAL Replay can take a long time. Increasing failureThreshold for ~30 min of time until killed
    failureThreshold: 60
    initialDelaySeconds: 120
    periodSeconds: 30
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  statefulStrategy:
    type: RollingUpdate

  terminationGracePeriodSeconds: 240

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

ruler:
  enabled: true
  replicas: 1

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 200m
  #    memory: 256Mi
  #  requests:
  #    cpu: 10m
  #    memory: 32Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity: {}
  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []
  # allow configuring rules via configmap in case you don't want to use the configs db
  directories: {}
    # tenant_foo:
    #   rules1.txt: |
    #     groups:
    #       - name: should_fire
    #         rules:
    #           - alert: HighPercentageError
    #             expr: |
    #               sum(rate({app="foo", env="production"} |= "error" [5m])) by (job)
    #                 /
    #               sum(rate({app="foo", env="production"}[5m])) by (job)
    #                 > 0.05
    #             for: 10m
    #             labels:
    #               severity: warning
    #             annotations:
    #               summary: High error rate


querier:
  replicas: 2

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 50m
  #    memory: 128Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - querier
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

query_frontend:
  replicas: 2

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 256Mi
  #  requests:
  #    cpu: 10m
  #    memory: 32Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - query-frontend
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

table_manager:
  replicas: 1

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 10m
  #    memory: 32Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity: {}
  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

configs:
  enabled: false
  replicas: 1

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 10m
  #    memory: 32Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity: {}
  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /ready
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 180

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

nginx:
  enabled: true
  replicas: 2
  http_listen_port: 80
  config:
    dnsResolver: kube-dns.kube-system.svc.cluster.local
    ## ref: http://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size
    client_max_body_size: 1M
    setHeaders: {}
      # X-Scope-OrgID: 0
  image:
    repository: nginx
    tag: 1.21
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 100m
  #    memory: 128Mi
  #  requests:
  #    cpu: 10m
  #    memory: 16Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: ''
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity: {}
  annotations: {}
  persistence:
    subPath:

  startupProbe:
    httpGet:
      path: /healthz
      port: http-metrics
    failureThreshold: 10
  livenessProbe:
    httpGet:
      path: /healthz
      port: http-metrics
  readinessProbe:
    httpGet:
      path: /healthz
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1

  terminationGracePeriodSeconds: 10

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

store_gateway:
  replicas: 1

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 100m
  #    memory: 512Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
  # log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - store-gateway
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}

  persistentVolume:
    ## If true Store-gateway will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Store-gateway data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## Store-gateway data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Store-gateway data Persistent Volume size
    ##
    size: 2Gi

    ## Subdirectory of Store-gateway data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ''

    ## Store-gateway data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  startupProbe:
    # Increasing failureThreshold for ~30 min of time until killed
    failureThreshold: 60
    initialDelaySeconds: 120
    periodSeconds: 30
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate

  terminationGracePeriodSeconds: 240

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

compactor:
  enabled: true
  replicas: 1

  service:
    annotations: {}
    labels: {}

  serviceMonitor:
    enabled: false
    additionalLabels: {}

  resources: {}
  #  limits:
  #    cpu: 1
  #    memory: 1Gi
  #  requests:
  #    cpu: 100m
  #    memory: 512Mi

  ## Additional Cortex container arguments, e.g. log level (debug, info, warn, error)
  extraArgs:
    {}
    #  log.level: debug

  ## Pod Labels
  podLabels: {}

  ## Pod Annotations
  podAnnotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: 'http-metrics'

  nodeSelector: {}
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: target
                  operator: In
                  values:
                    - compactor
            topologyKey: 'kubernetes.io/hostname'

  annotations: {}

  persistentVolume:
    ## If true compactor will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## compactor data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## compactor data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## compactor data Persistent Volume size
    ##
    size: 2Gi

    ## Subdirectory of compactor data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ''

    ## compactor data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"

  startupProbe:
    # Increasing failureThreshold for ~30 min of time until killed
    failureThreshold: 60
    initialDelaySeconds: 120
    periodSeconds: 30
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
      scheme: HTTP
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics

  securityContext: {}

  strategy:
    type: RollingUpdate

  terminationGracePeriodSeconds: 240

  tolerations: []
  podDisruptionBudget:
    maxUnavailable: 1
  initContainers: []
  extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  extraPorts: []
  env: []

# chunk caching
memcached:
  enabled: false
  architecture: "high-availability"
  replicaCount: 2
  pdbMinAvailable: 1
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true
  # tolerations: {}
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  # affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

# index read caching
memcached-index-read:
  enabled: false
  architecture: "high-availability"
  replicaCount: 2
  # pdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true
  # tolerations: []
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  # affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

# index write caching
memcached-index-write:
  enabled: false
  architecture: "high-availability"
  replicaCount: 2
  # dpdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true
  # tolerations: []
  # - key: "dedicated"
  #   operator: "Equal"
  #   value: "cortex-memcached"
  #   effect: "NoSchedule"
  # affinity: {}
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #         nodeSelectorTerms:
  #         - matchExpressions:
  #           - key: dedicated
  #             operator: In
  #             values:
  #             - cortex-memcached

memcached-frontend:
  enabled: false
  architecture: "high-availability"
  replicaCount: 2
  # dpdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true

memcached-blocks-index:
  # enabled/disabled via the tags.blocks-storage-memcached boolean
  architecture: "high-availability"
  replicaCount: 2
  # dpdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true

memcached-blocks:
  # enabled/disabled via the tags.blocks-storage-memcached boolean
  architecture: "high-availability"
  replicaCount: 2
  # dpdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true

memcached-blocks-metadata:
  # enabled/disabled via the tags.blocks-storage-memcached boolean
  architecture: "high-availability"
  replicaCount: 2
  # dpdbMinAvailable: 1
  # image: memcached:1.5.7-alpine
  memcached:
    maxItemMemory: 3840
    extraArgs:
      - -I 32m
    threads: 32
  resources: {}
  #  requests:
  #    memory: 1Gi
  #    cpu: 10m
  #  limits:
  #    memory: 4Gi
  #    cpu: 1
  metrics:
    enabled: true

configsdb_postgresql:
  enabled: false
  uri:
  auth:
    password:
    existing_secret:
      name:
      key:
